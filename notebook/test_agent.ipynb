{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph_sdk import get_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = get_client(url=\"http://localhost:8123\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic = \"Top 5 chip providers for LLM Training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"type\": \"object\",\n",
    "    \"properties\": {\n",
    "        \"companies\": {\n",
    "            \"type\": \"array\",\n",
    "            \"items\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"name\": {\"type\": \"string\", \"description\": \"Company name\"},\n",
    "                    \"technologies\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Brief summary of key technologies used by the company\",\n",
    "                    },\n",
    "                    \"market_share\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Overview of market share for this company\",\n",
    "                    },\n",
    "                    \"future_outlook\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Brief summary of future prospects and developments in the field for this company\",\n",
    "                    },\n",
    "                    \"key_powers\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Which of the 7 Powers (Scale Economies, Network Economies, Counter Positioning, Switching Costs, Branding, Cornered Resource, Process Power) best describe this company's competitive advantage\",\n",
    "                    },\n",
    "                },\n",
    "                \"required\": [\"name\", \"technologies\", \"market_share\", \"future_outlook\"],\n",
    "            },\n",
    "            \"description\": \"List of companies\",\n",
    "        }\n",
    "    },\n",
    "    \"required\": [\"companies\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========================\n",
      "=========================\n",
      "content='' additional_kwargs={'additional_kwargs': {'tool_calls': [{'id': 'call_kBOf74ZI5m8vDMvLFe1YaFao', 'function': {'arguments': '{\"query\":\"Top chip providers for LLM training 2023\"}', 'name': 'search'}, 'type': 'function', 'index': 0}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 20, 'prompt_tokens': 627, 'total_tokens': 647, 'completion_tokens_details': None}, 'model_name': 'openai/gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'tool_calls', 'logprobs': None}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 627, 'output_tokens': 20, 'total_tokens': 647}} response_metadata={} id='run-9ad6c10c-9de7-42d5-aef8-7073a6c6e378-0' tool_calls=[{'name': 'search', 'args': {'query': 'Top chip providers for LLM training 2023'}, 'id': 'call_kBOf74ZI5m8vDMvLFe1YaFao', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='[{\"url\": \"https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/\", \"content\": \"One Giant Superchip for LLMs, Recommenders, and GNNs: Introducing NVIDIA GH200 NVL32\\\\nAt AWS re:Invent 2023, AWS and NVIDIA announced that AWS will be the first cloud provider to offer NVIDIA GH200 Grace Hopper Superchips interconnected with NVIDIA NVLink technology through NVIDIA DGX Cloud and running on Amazon Elastic Compute Cloud (Amazon EC2). Supercomputing Platform on NVIDIA HGX A100 80GB GPUs\\\\nHGX-2 Fuses HPC and AI Computing Architectures\\\\nRelated posts\\\\nMastering LLM Techniques: Training\\\\nOptimize Energy Efficiency of Multi-Node VASP Simulations with NVIDIA Magnum IO\\\\nJust Released: NVIDIA HPC SDK 23.9\\\\nJust Released: NVIDIA HPC SDK v23.7\\\\nProgramming the Quantum-Classical Supercomputer NVIDIA GH200 NVL32 with Grace Hopper provides 7x the amount of fast-access memory compared to four HGX H100 and delivers 7x the bandwidth compared to the PCIe Gen5 connections to the GPU in conventional x86-based designs. NVIDIA GH200 NVL32 is supported by the NVIDIA HPC SDK and the full suite of CUDA, NVIDIA CUDA-X, and NVIDIA Magnum IO libraries, accelerating over 3,000 GPU applications.\\\\n The large memory space of NVIDIA GH200 NVL32 also improves operational efficiency, with the ability to store multiple models on the same node and quickly swap models in to maximize utilization.\\\\n\"}, {\"url\": \"https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e\", \"content\": \"In November 2023, we used Multislice Training to run what we believe to be the world\\'s largest publicly disclosed LLM distributed training job (in terms of the number of chips used for training) on a compute cluster of 50,944 Cloud TPU v5e chips (spanning 199 Cloud TPU v5e pods) that is capable of achieving 10 exa-FLOPs (16-bit), or 20 exa ...\"}, {\"url\": \"https://towardsai.net/p/cloud-computing/15-leading-cloud-providers-for-gpu-powered-llm-fine-tuning-and-training\", \"content\": \"Nvidia GPUs dominate market share, particularly with their A100 and H100 chips, but AMD has also grown its GPU offering, and companies like Google have built custom AI chips in-house (TPUs). Nvidia data center revenue (predominantly sale of GPUs for LLM use cases) grew 279% yearly in 3Q of 2023 to $14.5 billion!\"}, {\"url\": \"https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators\", \"content\": \"In Table 1a) we compare the IntelR GaudiR 2 against the NVIDIA A100 and AMD MI250, and in Table 1b) we compare the upcoming Intel Gaudi 3 against the NVIDIA H100/H200 and AMD MI300X.\\\\nThe IntelR GaudiR 2 accelerator ships in systems of 8x Gaudi 2 accelerators and has a unique scale-out networking design built on RDMA over Converged Ethernet (RoCEv2). Based on these public on-demand quoted prices from AWS and IDC, we found that the IntelR GaudiR 2 has the best training performance-per-dollar, with an average advantage of 4.8x vs the NVIDIA A100-80GB, 4.2x vs. the NVIDIA A100-40GB, and 5.19x vs. the NVIDIA H100. When comparing the IntelR GaudiR 2 against the same-generation NVIDIA A100 and AMD MI250 GPUs, we found that Gaudi 2 is a clear winner, with an average speedup of 1.22x vs. the A100-80GB, 1.34x vs. the A100-40GB, and 1.59x vs. the MI250.\\\\n These changes typically include substituting `tensor.cuda()` commands with `tensor.to(\\'hpu\\')` commands or using a PyTorch trainer like Composer or HF Trainer, which already has Gaudi support built in!\\\\nLLM Training Performance\\\\nFor single-node LLM training performance, we found that the IntelR GaudiR 2 is the 2nd-fastest AI chip on the market today, reaching over 260 TFLOP/s/device with SynapseAI 1.12 and BF16 mixed precision training. Given these results, in addition to Intel\\'s MLPerf Training 3.1 results training GPT3-175B on up to 384x Gaudi 2, we are optimistic about IntelR GaudiR 2 performance at higher device counts, and we look forward to sharing results on larger IntelR GaudiR\\xa0 2/Gaudi 3 clusters in the future.\\\\n\"}, {\"url\": \"https://www.assemblyai.com/blog/best-large-language-models-frameworks/\", \"content\": \"It offers a simple API for applying LLMs to up to 100 hours of audio data, even exposing endpoints for common use tasks It\\'s smart enough to auto-generate subtitles, identify speakers, and transcribe audio in real time.\\\\nBenefits of Using an LLM\\\\nLLMs might seem like a nice-to-have right now, but they\\'ll eventually become integral to our day-to-day processes and systems. Industry\\\\nLarge Language Models & Frameworks for Product Managers in 2023\\\\nGain the basic know-how you need to understand what a large language model (LLM) is, how it works, and the best models in 2023.\\\\n It is a framework for applying large language models to spoken data, allowing users to generate custom summaries, ask questions about the data, and obtain insights from nearly 100 hours of audio with just a single API call.\\\\n They do have a few challenges and limitations you\\'ll want to keep in mind:\\\\nWhile LLMs offer potential advantages in terms of scalability and cost-efficiency, they also present meaningful challenges, especially concerning data quality, biases, and ethical considerations.\\\\n The Best Large Language Models and Frameworks in 2023\\\\nStartups and enterprises alike are pushing the boundaries of what\\'s possible with large language models.\"}, {\"url\": \"https://blog.monsterapi.ai/blogs/top-open-source-llms-for-2024/\", \"content\": \"On the opposite end of the LLM spectrum are the open-source LLMs. As most LLMs are controlled by big tech such as Microsoft, Google, and Meta, Open-Source LLMs are a way for the general public to have access to generative AI. In this article, we\\'ve compiled a list of the top 8 open-source LLMs of 2024. Best 8 Open-Source LLMs for 2024\"}, {\"url\": \"https://www.techmonitor.ai/technology/companies-large-language-models-llms-chatgpt-alternatives\", \"content\": \"Then, on 27 June 2023, Baidu announced a further development to the LLM in the form of ERNIE 3.5, which it describes as having achieved \\\\\"broad enhancements in efficacy, functionality, and performance\\\\\" compared to its predecessor. In fact, Baidu claims a 17-fold increase in performance compared to 3.0 and argues that it outperforms GPT4.0 in ...\"}, {\"url\": \"https://www.techradar.com/computing/artificial-intelligence/best-llms\", \"content\": \"Released in February 2024, Gemini 1.5 is an updated version of the original Gemini LLM released in December 2023 that offers improved capabilities and performance compared to the original.\"}, {\"url\": \"https://www.lakera.ai/blog/open-source-llms\", \"content\": \"Here are the detail of this model:\\\\nParameters: 30B\\\\nLicense: Apache-2.0\\\\nRelease Date: June 22, 2023\\\\nGithub: https://github.com/mosaicml/llm-foundry/\\\\nHuggingFace: https://huggingface.co/mosaicml/mpt-30b\\\\nPaper: MPT-30B: Raising the bar for open-source foundation models\\\\nVariants: There are two models available: MPT-7B and MPT-30B. Each model comes with an instruction and a chat version.\\\\n Here are the details of this model:\\\\nParameters: 65B\\\\nLicense: MIT License\\\\nRelease Date: May 24, 2023\\\\nGithub: https://github.com/artidoro/qlora\\\\nPaper: QLoRA: Efficient Fine-tuning of Quantized LLMs\\\\nBLOOM, which stands for BigScience Language Open-science Open-access Multilingual, is a powerful language model that uses large computational resources to generate text based on a given prompt. Google\\'s LAN-T5 has been released in 5 variants: the flan-t5-small with 80M parameters, the flan-t5-base with 250M parameters, the flan-t5-large with 780M parameters, the flan-t5-xl boasting 3B parameters, and the largest, flan-t5-xxl, with 11B parameters.\\\\n Although this LLM is not the most advanced model available on the market, this LLM introduces the QLoRA method, which offers an efficient fine-tuning technique and enables personal and smaller businesses to fine-tune large models with up to 65 billion parameters.\\\\n Here are the details of this model:\\\\nParameters: 12B\\\\nLicense: permissive license (CC-BY-SA)\\\\nRelease Date: Apr 12, 2023\\\\nGithub: https://github.com/databrickslabs/dolly\\\\nHuggingFace: https://huggingface.co/databricks/dolly-v2-12b\\\\nPaper:\"}, {\"url\": \"https://research.aimultiple.com/ai-chip-makers/\", \"content\": \"Top 10 AI Chip Makers of 2023: In-depth Guide\\\\nAs the figure above illustrates, the number of parameters (consequently the width and depth) of the neural networks and therefore the model size is increasing. As of October 2023, the company’s long term viability is at risk as it is losing ~$200M per year and ~$160M in assets as of Jan 1st, 2023.1718\\\\nMythic\\\\nMythic was founded in 2012 and is focused on edge AI. Thanks to the generative AI boom, NVIDIA had excellent results in Q2 2023, reached a trillion in valuation and solidified its status as the leader of GPU and AI hardware markets.\\\\n If I’m not mistaken, the AI is also the second most widely tested and used in the “wild”, just below that of Google due to Google using it in their Search.\\\\n Microsoft Azure\\\\nMicrosoft is reported to be working on its own AI chips which are expected to be launched in 2023.16\\\\nGraphcore\\\\nGraphcore is a British company founded in 2016.\"}]' name='search' id='536b7391-86c3-4cb4-b001-027658b2d3d5' tool_call_id='call_kBOf74ZI5m8vDMvLFe1YaFao'\n",
      "=========================\n",
      "content='' additional_kwargs={'additional_kwargs': {'tool_calls': [{'id': 'call_VoeoCJ1h687jsGRDy5TcVCeQ', 'function': {'arguments': '{\"url\": \"https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/\"}', 'name': 'scrape_website'}, 'type': 'function', 'index': 0}, {'id': 'call_l83UzIYb5TDFqPRfSS5kOFrl', 'function': {'arguments': '{\"url\": \"https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e\"}', 'name': 'scrape_website'}, 'type': 'function', 'index': 1}, {'id': 'call_SSjMKOu6EOxlUSS95GRNAgxv', 'function': {'arguments': '{\"url\": \"https://towardsai.net/p/cloud-computing/15-leading-cloud-providers-for-gpu-powered-llm-fine-tuning-and-training\"}', 'name': 'scrape_website'}, 'type': 'function', 'index': 2}, {'id': 'call_M3d94AzNIl2jvf6QWfASW3SN', 'function': {'arguments': '{\"url\": \"https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators\"}', 'name': 'scrape_website'}, 'type': 'function', 'index': 3}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 190, 'prompt_tokens': 3007, 'total_tokens': 3197, 'completion_tokens_details': None}, 'model_name': 'openai/gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'tool_calls', 'logprobs': None}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 3007, 'output_tokens': 190, 'total_tokens': 3197}} response_metadata={} id='run-69784ad9-2658-47d9-a2ee-14e58732653a-0' tool_calls=[{'name': 'scrape_website', 'args': {'url': 'https://developer.nvidia.com/blog/one-giant-superchip-for-llms-recommenders-and-gnns-introducing-nvidia-gh200-nvl32/'}, 'id': 'call_VoeoCJ1h687jsGRDy5TcVCeQ', 'type': 'tool_call'}, {'name': 'scrape_website', 'args': {'url': 'https://cloud.google.com/blog/products/compute/the-worlds-largest-distributed-llm-training-job-on-tpu-v5e'}, 'id': 'call_l83UzIYb5TDFqPRfSS5kOFrl', 'type': 'tool_call'}, {'name': 'scrape_website', 'args': {'url': 'https://towardsai.net/p/cloud-computing/15-leading-cloud-providers-for-gpu-powered-llm-fine-tuning-and-training'}, 'id': 'call_SSjMKOu6EOxlUSS95GRNAgxv', 'type': 'tool_call'}, {'name': 'scrape_website', 'args': {'url': 'https://www.databricks.com/blog/llm-training-and-inference-intel-gaudi2-ai-accelerators'}, 'id': 'call_M3d94AzNIl2jvf6QWfASW3SN', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='Based on the content from the Databricks blog post regarding LLM training and inference with Intel Gaudi 2 AI accelerators, here are some notes:\\n\\n### Website Overview\\n- **Title**: LLM Training and Inference with Intel Gaudi 2 AI Accelerators\\n- **Author**: Databricks\\n- **Published Date**: January 4, 2024\\n- **Description**: The blog discusses benchmarking of LLM (Large Language Model) training and inference on an Intel Gaudi2 cluster, highlighting its performance in both single-node and multi-node configurations.\\n\\n### Key Points\\n- **Benchmarking**: The article emphasizes the results of benchmarking exercises, showcasing the effectiveness of Intel Gaudi 2 AI accelerators in handling LLM tasks.\\n- **Performance Metrics**: It mentions improvements in performance metrics over previous generations or competitors, indicating that Intel Gaudi 2 provides significant performance boosts for AI workloads.\\n- **Target Audience**: The content appears targeted at developers, data scientists, and IT professionals interested in AI model training and inference capabilities.\\n\\n### Content Features\\n- **Visuals**: The post includes images that likely illustrate performance comparisons or architecture diagrams of the Intel Gaudi 2 accelerator.\\n- **Technical Insights**: It may provide technical details, such as configuration settings that optimize performance, best practices for deployment, and insights into the architecture of the Gaudi 2 accelerators.\\n\\n### Future Implications\\n- **Future Outlook**: The article likely discusses future trends in AI hardware and software, particularly how advancements in accelerators like Gaudi 2 can shape the landscape of AI model training.\\n\\n### Conclusion\\nThe blog post serves as a resource for those looking to understand the capabilities of Intel Gaudi 2 AI accelerators in the context of LLM training and inference, providing both performance insights and practical guidance for implementation. \\n\\nThis overview can help in identifying Databricks as a leading company in AI and big data solutions, particularly in the context of developing and deploying LLM systems.' name='scrape_website' id='ad49ecbf-e124-4945-bcd5-320172695891' tool_call_id='call_M3d94AzNIl2jvf6QWfASW3SN'\n",
      "=========================\n",
      "content='' additional_kwargs={'additional_kwargs': {'tool_calls': [{'id': 'call_DeSopxCGzTB8IE7sSvDx1ByA', 'function': {'arguments': '{\"companies\":[{\"name\":\"NVIDIA\",\"key_powers\":\"Scale Economies\",\"market_share\":\"Dominates the GPU market for LLM training, with significant revenue growth in 2023 (279% YoY increase).\",\"technologies\":\"NVIDIA GH200 Grace Hopper Superchips, NVLink technology, A100 and H100 GPUs.\",\"future_outlook\":\"Expected to continue leading in AI hardware with advancements in superchip technology and cloud integrations.\"},{\"name\":\"Google Cloud\",\"key_powers\":\"Network Economies\",\"market_share\":\"A significant player in the cloud services market, competing with AWS and Azure for AI and machine learning applications.\",\"technologies\":\"TPU v5e chips, advanced distributed training infrastructure.\",\"future_outlook\":\"Continued growth in demand for cloud-based AI solutions as businesses leverage machine learning.\"},{\"name\":\"Intel\",\"key_powers\":\"Process Power\",\"market_share\":\"Increasing share in AI accelerator market with Gaudi series chips, positioned as a strong competitor to NVIDIA.\",\"technologies\":\"Intel Gaudi 2 AI accelerators, designed for high efficiency in LLM training.\",\"future_outlook\":\"Positive outlook with ongoing development of Gaudi 3 and further enhancements in AI training capabilities.\"},{\"name\":\"AMD\",\"key_powers\":\"Counter Positioning\",\"market_share\":\"Growing presence in the GPU market, particularly with their MI series chips, competing against NVIDIA.\",\"technologies\":\"AMD MI250 and MI300X GPUs, focusing on AI and machine learning applications.\",\"future_outlook\":\"Anticipated growth in the AI sector as demand for efficient GPUs rises.\"},{\"name\":\"AWS\",\"key_powers\":\"Scale Economies\",\"market_share\":\"Significant market share in cloud services, leveraging partnerships with NVIDIA for advanced AI services.\",\"technologies\":\"Integration of NVIDIA superchips into their EC2 offerings.\",\"future_outlook\":\"Expected to expand AI service offerings with new technologies and partnerships.\"}]}', 'name': 'Info'}, 'type': 'function', 'index': 0}], 'refusal': None}, 'response_metadata': {'token_usage': {'completion_tokens': 385, 'prompt_tokens': 4907, 'total_tokens': 5292, 'completion_tokens_details': None}, 'model_name': 'openai/gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'tool_calls', 'logprobs': None}, 'example': False, 'invalid_tool_calls': [], 'usage_metadata': {'input_tokens': 4907, 'output_tokens': 385, 'total_tokens': 5292}} response_metadata={} id='run-a0abcaf8-1205-4f60-8f80-9707f13d5a51-0' tool_calls=[{'name': 'Info', 'args': {'companies': [{'name': 'NVIDIA', 'key_powers': 'Scale Economies', 'market_share': 'Dominates the GPU market for LLM training, with significant revenue growth in 2023 (279% YoY increase).', 'technologies': 'NVIDIA GH200 Grace Hopper Superchips, NVLink technology, A100 and H100 GPUs.', 'future_outlook': 'Expected to continue leading in AI hardware with advancements in superchip technology and cloud integrations.'}, {'name': 'Google Cloud', 'key_powers': 'Network Economies', 'market_share': 'A significant player in the cloud services market, competing with AWS and Azure for AI and machine learning applications.', 'technologies': 'TPU v5e chips, advanced distributed training infrastructure.', 'future_outlook': 'Continued growth in demand for cloud-based AI solutions as businesses leverage machine learning.'}, {'name': 'Intel', 'key_powers': 'Process Power', 'market_share': 'Increasing share in AI accelerator market with Gaudi series chips, positioned as a strong competitor to NVIDIA.', 'technologies': 'Intel Gaudi 2 AI accelerators, designed for high efficiency in LLM training.', 'future_outlook': 'Positive outlook with ongoing development of Gaudi 3 and further enhancements in AI training capabilities.'}, {'name': 'AMD', 'key_powers': 'Counter Positioning', 'market_share': 'Growing presence in the GPU market, particularly with their MI series chips, competing against NVIDIA.', 'technologies': 'AMD MI250 and MI300X GPUs, focusing on AI and machine learning applications.', 'future_outlook': 'Anticipated growth in the AI sector as demand for efficient GPUs rises.'}, {'name': 'AWS', 'key_powers': 'Scale Economies', 'market_share': 'Significant market share in cloud services, leveraging partnerships with NVIDIA for advanced AI services.', 'technologies': 'Integration of NVIDIA superchips into their EC2 offerings.', 'future_outlook': 'Expected to expand AI service offerings with new technologies and partnerships.'}]}, 'id': 'call_DeSopxCGzTB8IE7sSvDx1ByA', 'type': 'tool_call'}]\n",
      "=========================\n",
      "content='The provided information includes the names, competitive advantages, market share, technologies, and future outlook for each of the top 5 chip providers for LLM training, fulfilling the requirements outlined in the initial request.\\nThe details about each company are specific and relevant, highlighting their technologies and market positions, which are crucial for understanding their roles in the LLM training landscape.\\nThe information is concise and structured, making it easy to read and understand, which is important for conveying complex technical details.' name='Info' id='70f03b2a-2a5a-4379-ae8e-8a49f05be122' tool_call_id='call_DeSopxCGzTB8IE7sSvDx1ByA'\n",
      "=========================\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import convert_to_messages\n",
    "\n",
    "# Thread\n",
    "thread = await client.threads.create()\n",
    "\n",
    "# Model\n",
    "llm = \"anthropic/claude-3-5-sonnet-20240620\"\n",
    "llm = \"openai/gpt-4o\"\n",
    "\n",
    "# Stream\n",
    "async for event in client.runs.stream(\n",
    "    thread[\"thread_id\"],\n",
    "    assistant_id=\"agent\",\n",
    "    input={\n",
    "        \"topic\": topic,\n",
    "        \"extraction_schema\": schema,\n",
    "        \"configurable\": {\"model_name\": llm},\n",
    "    },\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    messages = event.data.get(\"messages\", None)\n",
    "    if messages:\n",
    "        print(convert_to_messages(messages)[-1])\n",
    "    print(\"=\" * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_state = await client.threads.get_state(thread[\"thread_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Top 5 Chip Providers for LLM Training\n",
       "\n",
       "\n",
       "## NVIDIA\n",
       "\n",
       "**Key Technologies:** NVIDIA GH200 Grace Hopper Superchips, NVLink technology, A100 and H100 GPUs.\n",
       "\n",
       "**Market Share:** Dominates the GPU market for LLM training, with significant revenue growth in 2023 (279% YoY increase).\n",
       "\n",
       "**Key Powers:** Scale Economies\n",
       "\n",
       "**Future Outlook:** Expected to continue leading in AI hardware with advancements in superchip technology and cloud integrations.\n",
       "\n",
       "---\n",
       "\n",
       "## Google Cloud\n",
       "\n",
       "**Key Technologies:** TPU v5e chips, advanced distributed training infrastructure.\n",
       "\n",
       "**Market Share:** A significant player in the cloud services market, competing with AWS and Azure for AI and machine learning applications.\n",
       "\n",
       "**Key Powers:** Network Economies\n",
       "\n",
       "**Future Outlook:** Continued growth in demand for cloud-based AI solutions as businesses leverage machine learning.\n",
       "\n",
       "---\n",
       "\n",
       "## Intel\n",
       "\n",
       "**Key Technologies:** Intel Gaudi 2 AI accelerators, designed for high efficiency in LLM training.\n",
       "\n",
       "**Market Share:** Increasing share in AI accelerator market with Gaudi series chips, positioned as a strong competitor to NVIDIA.\n",
       "\n",
       "**Key Powers:** Process Power\n",
       "\n",
       "**Future Outlook:** Positive outlook with ongoing development of Gaudi 3 and further enhancements in AI training capabilities.\n",
       "\n",
       "---\n",
       "\n",
       "## AMD\n",
       "\n",
       "**Key Technologies:** AMD MI250 and MI300X GPUs, focusing on AI and machine learning applications.\n",
       "\n",
       "**Market Share:** Growing presence in the GPU market, particularly with their MI series chips, competing against NVIDIA.\n",
       "\n",
       "**Key Powers:** Counter Positioning\n",
       "\n",
       "**Future Outlook:** Anticipated growth in the AI sector as demand for efficient GPUs rises.\n",
       "\n",
       "---\n",
       "\n",
       "## AWS\n",
       "\n",
       "**Key Technologies:** Integration of NVIDIA superchips into their EC2 offerings.\n",
       "\n",
       "**Market Share:** Significant market share in cloud services, leveraging partnerships with NVIDIA for advanced AI services.\n",
       "\n",
       "**Key Powers:** Scale Economies\n",
       "\n",
       "**Future Outlook:** Expected to expand AI service offerings with new technologies and partnerships.\n",
       "\n",
       "---\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "\n",
    "def format_llm_chip_info(data):\n",
    "    markdown_text = \"# Top 5 Chip Providers for LLM Training\\n\\n\"\n",
    "\n",
    "    for company in data[\"companies\"]:\n",
    "        markdown_text += f\"\"\"\n",
    "## {company['name']}\n",
    "\n",
    "**Key Technologies:** {company['technologies']}\n",
    "\n",
    "**Market Share:** {company['market_share']}\n",
    "\n",
    "**Key Powers:** {company.get('key_powers', 'Not specified')}\n",
    "\n",
    "**Future Outlook:** {company['future_outlook']}\n",
    "\n",
    "---\n",
    "\"\"\"\n",
    "\n",
    "    return Markdown(markdown_text)\n",
    "\n",
    "\n",
    "# Display the formatted markdown\n",
    "display(format_llm_chip_info(current_state[\"values\"][\"info\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
